{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jordan-dsouza/Projects/blob/main/WebScrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhJe7cdzaYZ3"
      },
      "source": [
        "Requests sends HTTP requests.<br>\n",
        "BeautifulSoup parses HTML and XML documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDeLvzQEvMyp"
      },
      "source": [
        "#**Import libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVXYUJboVHD8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import shutil\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKHD2rjBoQs5",
        "outputId": "a08030ef-98b5-4680-de50-445614b95dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install syllables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geIaZfI9famK",
        "outputId": "b97c2f62-314c-4a90-87e8-600992b9ee70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting syllables\n",
            "  Downloading syllables-1.0.9-py3-none-any.whl (15 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
            "  Downloading cmudict-1.0.18-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<7.0,>=5.1 (from syllables)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (6.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=5.1->syllables) (3.17.0)\n",
            "Installing collected packages: importlib-metadata, cmudict, syllables\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed cmudict-1.0.18 importlib-metadata-6.11.0 syllables-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7bMNwy1ZE1Y"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"Input.xlsx\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJyS5p2vaQA1"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDnKWjDksmo3"
      },
      "source": [
        "#Extract Article Text:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function definition:"
      ],
      "metadata": {
        "id": "1BwK62K3TjPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt_SqJ4Fso0m"
      },
      "outputs": [],
      "source": [
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        #Extract article title:\n",
        "        title = soup.find('title').text\n",
        "\n",
        "        #Find main content container:\n",
        "        article_content = soup.find('div', class_=['td-post-content', 'tagdiv-type'])  #First class\n",
        "        if article_content is None:\n",
        "            article_content = soup.find('div', class_=['tdb-block-inner', 'td-fix-index'])  #Second class\n",
        "\n",
        "        if article_content:\n",
        "            #Extract text from para within main content container:\n",
        "            article_text = \"\"\n",
        "            paragraphs = article_content.find_all('p')\n",
        "            for paragraph in paragraphs:\n",
        "                article_text += paragraph.text + \"\\n\"\n",
        "         # Include text from <li> tags\n",
        "            for li_item in article_content.find_all('li'):\n",
        "                article_text += li_item.text + \"\\n\"\n",
        "\n",
        "\n",
        "            return title, article_text\n",
        "        else:\n",
        "            print(f\"No article content found for URL: {url}\")\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting article from {url}: {e}\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output folder with extracted text:"
      ],
      "metadata": {
        "id": "7fKysVgKToDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Folder to save the articles if it doesn't exist:\n",
        "output_folder = \"extracted_articles\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)"
      ],
      "metadata": {
        "id": "s_TVmUZJYBku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterate over each row in the DataFrame:\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row[\"URL_ID\"]\n",
        "    url = row[\"URL\"]\n",
        "\n",
        "    #Extract title and text using function extract_article_text:\n",
        "    title, text = extract_article_text(url)\n",
        "\n",
        "    if title and text:\n",
        "        #Save into txt file in the output folder:\n",
        "        file_path = os.path.join(output_folder, f\"{url_id}.txt\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"Title: {title}\\n\\n\")\n",
        "            file.write(text)\n",
        "            print(f\"Article title and text saved to {file_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to extract article title and text for URL_ID: {url_id}\")"
      ],
      "metadata": {
        "id": "BFq1Kzvtjhyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optional: Create zip file for extracted folder."
      ],
      "metadata": {
        "id": "jyeik9FFTwHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Define the folder path to be zipped\n",
        "folder_path = \"/content/extracted_articles\"\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    # Define the name for the zip file\n",
        "    zip_file_name = \"ExtractedArticles\"\n",
        "\n",
        "    # Zip the folder\n",
        "    zip_file_path = shutil.make_archive(\"/content/\" + zip_file_name, 'zip', folder_path)\n",
        "\n",
        "    # Check if the zip file was created successfully\n",
        "    if os.path.exists(zip_file_path):\n",
        "        # Download the zip file\n",
        "        files.download(zip_file_path)\n",
        "    else:\n",
        "        print(\"Error: Zip file creation failed.\")\n",
        "else:\n",
        "    print(\"Error: Folder not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DAtXAvwTjJzk",
        "outputId": "9b37affb-89f0-435c-88bb-575ad1ecffb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_907e7f48-27c6-42a5-9aeb-9e0ff1b14725\", \"ExtractedArticles.zip\", 316460)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8idm2wEyN4S"
      },
      "source": [
        "#**Text analysis:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import stopwords.zip."
      ],
      "metadata": {
        "id": "e23rYx_eT_W6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P2ErT2A3RYm"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the uploaded zip file\n",
        "zip_file_path = '/content/StopWords.zip'\n",
        "\n",
        "# Directory to extract the contents of the zip file\n",
        "extract_folder = 'stop_words_folder'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stopwords list:"
      ],
      "metadata": {
        "id": "KUtZrv5i7Mja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxwiHv1S5_WR"
      },
      "outputs": [],
      "source": [
        "#Function to extract stop words from a file:\n",
        "def extract_stop_words(file_path):\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        stop_words = file.read().splitlines()\n",
        "    return set(stop_words)\n",
        "\n",
        "#Function to remove stop words from text\n",
        "def remove_stop_words(text, stop_words):\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "#Folder with stop words files:\n",
        "stop_words_folder = '/content/stop_words_folder/StopWords/StopWords'\n",
        "\n",
        "#Load stop words from all files in the folder:\n",
        "stop_words_files = [os.path.join(stop_words_folder, file) for file in os.listdir(stop_words_folder)]\n",
        "stop_words_list_upper = [extract_stop_words(file) for file in stop_words_files]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words_list_upper)"
      ],
      "metadata": {
        "id": "8tf3KRkea8hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list = [word.lower() for stop_words_set in stop_words_list_upper for word in stop_words_set]\n",
        "print(stop_words_list)"
      ],
      "metadata": {
        "id": "2vctjdDgdUFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stop_words_list))"
      ],
      "metadata": {
        "id": "h4fwgpjJlYaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clean the text:"
      ],
      "metadata": {
        "id": "jL4Pfxu5V22x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "nltk.download('punkt')\n",
        "#Path to the folder containing articles:\n",
        "articles_folder_path = \"/content/extracted_articles\"\n",
        "\n",
        "#Function to clean the text using stopwords:\n",
        "def clean_text(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [word for word in word_tokens if word.lower() not in stop_words_list]\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "    return cleaned_text\n",
        "\n",
        "#Folder to store cleaned articles:\n",
        "cleaned_articles_folder = \"/content/cleaned_articles\"\n",
        "os.makedirs(cleaned_articles_folder, exist_ok=True)\n",
        "\n",
        "#Process each article, clean text, and save to the cleaned_articles folder:\n",
        "for article_file in os.listdir(articles_folder_path):\n",
        "    article_path = os.path.join(articles_folder_path, article_file)\n",
        "    with open(article_path, 'r') as file:\n",
        "        article_text = file.read()\n",
        "        cleaned_text = clean_text(article_text)\n",
        "\n",
        "    #Save the cleaned text to a new file in the cleaned articles folder:\n",
        "    cleaned_article_path = os.path.join(cleaned_articles_folder, article_file)\n",
        "    with open(cleaned_article_path, 'w') as file:\n",
        "        file.write(cleaned_text)\n",
        "\n",
        "#Zip the cleaned articles folder:\n",
        "shutil.make_archive(\"/content/cleaned_articles\", 'zip', cleaned_articles_folder)\n",
        "\n",
        "# Download the zip file\n",
        "#files.download(\"/content/cleaned_articles.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "CX6TRgLnl5kp",
        "outputId": "d8a9c5a8-974e-408b-ea73-b3d7317d1788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/cleaned_articles.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#files.download(\"/content/cleaned_articles.zip\")"
      ],
      "metadata": {
        "id": "DUgY32Mom6zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Positive Negative:**"
      ],
      "metadata": {
        "id": "330QsLYKo8Ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. POSITIVE SCORE\n",
        "2. NEGATIVE SCORE\n",
        "3. POLARITY SCORE\n",
        "4. SUBJECTIVITY SCORE"
      ],
      "metadata": {
        "id": "US1p2Za3WOup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unzip MasterDictionary:"
      ],
      "metadata": {
        "id": "WmnYQWpBpXE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Path to the uploaded zip file:\n",
        "zip_file_path = '/content/MasterDictionary.zip'\n",
        "\n",
        "#Directory to extract the contents of the zip file:\n",
        "extract_folder = 'MasterDictionary'\n",
        "\n",
        "#Extract the contents of the zip file:\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n"
      ],
      "metadata": {
        "id": "K5Z1c55qpWUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Average Sentence Length\n",
        "2. Percentage of Complex Words\n",
        "3. Fog Index\n",
        "4. Average Number of Words per Sentence\n",
        "5. Complex Words Count"
      ],
      "metadata": {
        "id": "RcqbTnEVPU9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def count_syllables_per_word(word):\n",
        "    #Remove common suffixes that do not contribute to syllable count:\n",
        "    suffixes = [\"es\", \"ed\", \"ness\", \"er\", \"est\", \"ing\", \"ly\", \"ful\", \"ment\", \"tion\"]\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "\n",
        "    #Count the number of vowels:\n",
        "    vowels = \"aeiou\"\n",
        "    syllable_count = sum(1 for i in range(len(word)) if word[i].lower() in vowels and (i == 0 or word[i-1].lower() not in vowels))\n",
        "\n",
        "    #When 'e' at the end is silent:\n",
        "    if word.endswith(\"e\") and syllable_count > 1:\n",
        "        syllable_count -= 1\n",
        "\n",
        "    #Words with no vowels:\n",
        "    if syllable_count == 0:\n",
        "        syllable_count = 1\n",
        "\n",
        "    return syllable_count\n",
        "\n",
        "# Define the folder containing the cleaned articles\n",
        "folder_path = \"/content/cleaned_articles\"\n",
        "\n",
        "# Create an empty list to store the results\n",
        "results = []\n",
        "\n",
        "#Iterate over each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    #Check if the file is a text file:\n",
        "    if filename.endswith(\".txt\"):\n",
        "\n",
        "        with open(os.path.join(folder_path, filename), \"r\") as file:\n",
        "            #Read file contents:\n",
        "            text = file.read()\n",
        "            #Tokenize the text into sentences:\n",
        "            sentences = sent_tokenize(text)\n",
        "            #Tokenize the text into words:\n",
        "            words = nltk.word_tokenize(text)\n",
        "            #Count the number of words:\n",
        "            num_words = len(words)\n",
        "            #Count the number of sentences:\n",
        "            num_sentences = len(sentences)\n",
        "\n",
        "            #Count the number of complex words:\n",
        "            num_complex_words = sum(count_syllables_per_word(word) > 2 for word in words)\n",
        "\n",
        "            #Average sentence length:\n",
        "            avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "            #Average number of words per sentence:\n",
        "            avg_words_per_sentence = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "            #Percentage of complex words:\n",
        "            percentage_complex_words = num_complex_words / num_words if num_words > 0 else 0\n",
        "\n",
        "            #Fog Index:\n",
        "            fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "            #Append the filename and metrics to the results list:\n",
        "            results.append({\"Filename\": filename,\n",
        "                            \"Average Sentence Length\": avg_sentence_length,\n",
        "                            \"Percentage of Complex Words\": percentage_complex_words,\n",
        "                            \"Fog Index\": fog_index,\n",
        "                            \"Average Number of Words per Sentence\": avg_words_per_sentence,\n",
        "                            \"Complex Words Count\": num_complex_words})\n",
        "\n",
        "\n",
        "df_results56789 = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "IC8NnyRlskqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word count"
      ],
      "metadata": {
        "id": "bBHvU8Kk2NbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and count words in text\n",
        "def count_clean_words(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Remove punctuation\n",
        "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
        "\n",
        "    # Remove empty strings\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    # Count the cleaned words\n",
        "    num_words = len(words)\n",
        "\n",
        "    return num_words\n",
        "\n",
        "# Define the folder containing the cleaned articles\n",
        "folder_path = \"/content/cleaned_articles\"\n",
        "\n",
        "# Create an empty list to store the results\n",
        "task10 = []\n",
        "\n",
        "# Iterate over each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    # Check if the file is a text file\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Open the file\n",
        "        with open(os.path.join(folder_path, filename), \"r\") as file:\n",
        "            # Read the contents of the file\n",
        "            text = file.read()\n",
        "            # Count the cleaned words in the text\n",
        "            num_clean_words = count_clean_words(text)\n",
        "            # Append the filename and cleaned word count to the results list\n",
        "            task10.append({\"Filename\": filename, \"Word Count\": num_clean_words})\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df_task10 = pd.DataFrame(task10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs0NV3070mQo",
        "outputId": "2f6c2a35-f09d-4f27-d804-2279af6b256d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort the DataFrame by the \"Filename\" column:\n",
        "df_task10.sort_values(by=\"Filename\", inplace=True)\n",
        "\n",
        "#Reset the index:\n",
        "df_task10.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Display the sorted DataFrame:\n",
        "df_task10.head()"
      ],
      "metadata": {
        "id": "yg9Lq6MB3JGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Personal Pronouns:"
      ],
      "metadata": {
        "id": "G17jybs_4TlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    # Define the list of personal pronouns\n",
        "    personal_pronouns = ['I', 'me', 'my', 'mine', 'myself',\n",
        "                         'we', 'us', 'our', 'ours', 'ourselves',\n",
        "                         'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "                         'he', 'him', 'his', 'himself',\n",
        "                         'she', 'her', 'hers', 'herself',\n",
        "                         'it', 'its', 'itself',\n",
        "                         'they', 'them', 'their', 'theirs', 'themselves']\n",
        "\n",
        "    # Define the regex pattern to match personal pronouns\n",
        "    pattern = r'\\b(?:{})\\b'.format('|'.join(personal_pronouns))\n",
        "\n",
        "    # Find all matches of the pattern in the text\n",
        "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Exclude matches where \"US\" is a country name\n",
        "    matches = [match for match in matches if match.lower() != \"us\"]\n",
        "\n",
        "    # Count the number of matches\n",
        "    count = len(matches)\n",
        "\n",
        "    return count\n",
        "\n",
        "# Define the folder containing the cleaned articles\n",
        "folder_path = \"/content/cleaned_articles\"\n",
        "\n",
        "# Create an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    # Check if the file is a text file\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Open the file\n",
        "        with open(os.path.join(folder_path, filename), \"r\") as file:\n",
        "            # Read the contents of the file\n",
        "            text = file.read()\n",
        "            # Count personal pronouns in the text\n",
        "            pronoun_count = count_personal_pronouns(text)\n",
        "            # Append the filename and pronoun count to the results list\n",
        "            results.append({\"Filename\": filename, \"Personal Pronoun Count\": pronoun_count})\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df_personal_pronouns = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "hikJY7Up4FCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort the DataFrame by the \"Filename\" column:\n",
        "df_personal_pronouns.sort_values(by=\"Filename\", inplace=True)\n",
        "\n",
        "#Reset the index:\n",
        "df_personal_pronouns.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Display the sorted DataFrame:\n",
        "df_personal_pronouns.head()"
      ],
      "metadata": {
        "id": "hKC965n84NTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Syllable count per word:"
      ],
      "metadata": {
        "id": "kg-zAP6B5JEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PosNeg"
      ],
      "metadata": {
        "id": "0pBg5HfRjTvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Positive Words:\n",
        "with open(os.path.join(\"MasterDictionary/MasterDictionary\", \"positive-words.txt\"), \"r\", encoding = \"latin-1\") as f:\n",
        "    positive_words = set(f.read().splitlines())\n",
        "#Load Negative Words:\n",
        "with open(os.path.join(\"MasterDictionary/MasterDictionary\", \"negative-words.txt\"), \"r\", encoding = \"latin-1\") as f:\n",
        "    negative_words = set(f.read().splitlines())\n",
        "\n",
        "#Path to the directory containing the cleaned articles:\n",
        "directory_path = \"cleaned_articles\"\n",
        "#Empty DataFrame to store scores:\n",
        "scores_df = pd.DataFrame(columns=[\"Filename\", \"Positive Score\", \"Negative Score\", \"Polarity Score\", \"Subjectivity Score\"])\n",
        "\n",
        "#Loop over each file in the directory:\n",
        "for filename in os.listdir(directory_path):\n",
        "    file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "    #Read text from file:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    #Tokenize the text:\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    #Count positive and negative words:\n",
        "    positive_count = sum(1 for word in words if word in positive_words)\n",
        "    negative_count = sum(-1 for word in words if word in negative_words)\n",
        "\n",
        "    #Calculate positive score and negative score:\n",
        "    #Negative score multiplied by -1 so score is +ve:\n",
        "    positive_score = positive_count\n",
        "    negative_score = -negative_count\n",
        "\n",
        "    # Calculate polarity score\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "\n",
        "    # Calculate subjectivity score\n",
        "    total_words = len(words)\n",
        "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
        "\n",
        "    scores_df = scores_df.append({\n",
        "        \"Filename\": filename,\n",
        "        \"Positive Score\": positive_score,\n",
        "        \"Negative Score\": negative_score,\n",
        "        \"Polarity Score\": polarity_score,\n",
        "        \"Subjectivity Score\": subjectivity_score\n",
        "    }, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "YZcj_P-WpCph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort the DataFrame by the \"Filename\" column:\n",
        "scores_df.sort_values(by=\"Filename\", inplace=True)\n",
        "\n",
        "#Reset the index:\n",
        "scores_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Display the sorted DataFrame:\n",
        "scores_df.head()"
      ],
      "metadata": {
        "id": "kCbyY_VBrCkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the cleaned articles\n",
        "folder_path = \"/content/cleaned_articles\"\n",
        "\n",
        "# Create an empty list to store the results\n",
        "results1 = []\n",
        "\n",
        "import syllables\n",
        "\n",
        "# Iterate over each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    # Check if the file is a text file\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Open the file\n",
        "        with open(os.path.join(folder_path, filename), \"r\") as file:\n",
        "            # Read the contents of the file\n",
        "            text = file.read()\n",
        "            # Tokenize the text into words\n",
        "            wordstk = re.findall(r'\\b\\w+\\b', text)\n",
        "            # Count syllables for each word\n",
        "#            syllable_counts = [syllables.estimate(word) for word in wordstk]\n",
        "            syllable_counts = [count_syllables_per_word(word) for word in wordstk]\n",
        "            # Calculate total syllable count for the file\n",
        "            total_syllables = sum(syllable_counts)\n",
        "            # Append the filename and total syllable count to the results list\n",
        "            results1.append({\"Filename\": filename, \"Total Syllable Count\": total_syllables})\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df_syllable_count = pd.DataFrame(results1)\n"
      ],
      "metadata": {
        "id": "Eywok3qp5OFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort the DataFrame by the \"Filename\" column:\n",
        "df_syllable_count.sort_values(by=\"Filename\", inplace=True)\n",
        "\n",
        "#Reset the index:\n",
        "df_syllable_count.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Display the sorted DataFrame:\n",
        "df_syllable_count.head()"
      ],
      "metadata": {
        "id": "_lh14sXA6geY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**FINAL FUNCTIONS TOGETHER:**"
      ],
      "metadata": {
        "id": "pO2NM78-slrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Positive Words:\n",
        "with open(os.path.join(\"MasterDictionary/MasterDictionary\", \"positive-words.txt\"), \"r\", encoding = \"latin-1\") as f:\n",
        "    positive_words = set(f.read().splitlines())\n",
        "#Load Negative Words:\n",
        "with open(os.path.join(\"MasterDictionary/MasterDictionary\", \"negative-words.txt\"), \"r\", encoding = \"latin-1\") as f:\n",
        "    negative_words = set(f.read().splitlines())\n",
        "\n",
        "\n",
        "\n",
        "def count_syllables_per_word(word):\n",
        "    #Remove common suffixes that do not contribute to syllable count:\n",
        "    suffixes = [\"es\", \"ed\", \"ness\", \"er\", \"est\", \"ing\", \"ly\", \"ful\", \"ment\", \"tion\"]\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            word = word[:-len(suffix)]\n",
        "\n",
        "    #Count the number of vowels:\n",
        "    vowels = \"aeiou\"\n",
        "    syllable_count = sum(1 for i in range(len(word)) if word[i].lower() in vowels and (i == 0 or word[i-1].lower() not in vowels))\n",
        "\n",
        "    #When 'e' at the end is silent:\n",
        "    if word.endswith(\"e\") and syllable_count > 1:\n",
        "        syllable_count -= 1\n",
        "\n",
        "    #Words with no vowels:\n",
        "    if syllable_count == 0:\n",
        "        syllable_count = 1\n",
        "\n",
        "    return syllable_count\n",
        "\n",
        "\n",
        "# Clean and count words in text\n",
        "def count_clean_words(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Remove punctuation\n",
        "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
        "\n",
        "    # Remove empty strings\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    # Count the cleaned words\n",
        "    num_words = len(words)\n",
        "\n",
        "    return num_words\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "\n",
        "    personal_pronouns = ['I', 'me', 'my', 'mine', 'myself',\n",
        "                         'we', 'us', 'our', 'ours', 'ourselves',\n",
        "                         'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "                         'he', 'him', 'his', 'himself',\n",
        "                         'she', 'her', 'hers', 'herself',\n",
        "                         'it', 'its', 'itself',\n",
        "                         'they', 'them', 'their', 'theirs', 'themselves']\n",
        "\n",
        "    # Define the regex pattern to match personal pronouns\n",
        "    pattern = r'\\b(?:{})\\b'.format('|'.join(personal_pronouns))\n",
        "\n",
        "    # Find all matches of the pattern in the text\n",
        "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Exclude matches where \"US\" is a country name\n",
        "    matches = [match for match in matches if match.lower() != \"us\"]\n",
        "\n",
        "    # Count the number of matches\n",
        "    count = len(matches)\n",
        "\n",
        "    return count"
      ],
      "metadata": {
        "id": "larZWQ0ZYNP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the cleaned articles\n",
        "folder_path = \"/content/cleaned_articles\"\n",
        "\n",
        "# Create an empty list to store the results\n",
        "results = []\n",
        "\n",
        "\n",
        "# Iterate over each file in the folder:\n",
        "for filename in os.listdir(folder_path):\n",
        "    # Check if the file is a text file\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Open the file\n",
        "        with open(os.path.join(folder_path, filename), \"r\") as file:\n",
        "            # Read the contents of the file\n",
        "            text = file.read()\n",
        "            #Tokenize the text into words:\n",
        "            words = word_tokenize(text.lower())  # Convert text to lowercase for consistency\n",
        "\n",
        "            #Tokenize the text into sentences:\n",
        "            sentences = sent_tokenize(text)\n",
        "\n",
        "            #Count positive (+1) and negative (-1) words:\n",
        "            positive_count = sum(1 for word in words if word in positive_words)\n",
        "            negative_count = sum(-1 for word in words if word in negative_words)\n",
        "\n",
        "            #Calculate positive score and negative score:\n",
        "            #Negative score multiplied by -1 so score is +ve:\n",
        "            positive_score = positive_count\n",
        "            negative_score = -negative_count\n",
        "\n",
        "            #Calculate polarity score:\n",
        "            polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "\n",
        "            #Calculate subjectivity score:\n",
        "            total_words = len(words)\n",
        "            subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
        "\n",
        "            #Count the number of sentences:\n",
        "            num_sentences = len(sentences)\n",
        "\n",
        "            #Count the number of complex words:\n",
        "            num_complex_words = sum(count_syllables_per_word(word) > 2 for word in words)\n",
        "\n",
        "            #Average sentence length:\n",
        "            avg_sentence_length = total_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "            #Average number of words per sentence:\n",
        "            avg_words_per_sentence = total_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "            #Percentage of complex words:\n",
        "            percentage_complex_words = num_complex_words / total_words if total_words > 0 else 0\n",
        "\n",
        "            #Fog Index:\n",
        "            fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "            #Count the cleaned words in the text:\n",
        "            num_clean_words = count_clean_words(text)\n",
        "\n",
        "            #Count personal pronouns in the text:\n",
        "            pronoun_count = count_personal_pronouns(text)\n",
        "\n",
        "            # Tokenize the text into words\n",
        "            # Count syllables for each word\n",
        "            #syllable_counts = [syllables.estimate(word) for word in words]\n",
        "            syllable_counts = [count_syllables_per_word(word) for word in words]\n",
        "            # Calculate total syllable count for the file\n",
        "            total_syllables = sum(syllable_counts)\n",
        "\n",
        "            results.append({\"Filename\": filename,\n",
        "                       \"Positive Score\": positive_score,\n",
        "                       \"Negative Score\": negative_score,\n",
        "                       \"Polarity Score\": polarity_score,\n",
        "                       \"Subjectivity Score\": subjectivity_score,\n",
        "                       \"Average Sentence Length\": avg_sentence_length,\n",
        "                       \"Percentage of Complex Words\": percentage_complex_words,\n",
        "                        \"Fog Index\": fog_index,\n",
        "                        \"Average Number of Words per Sentence\": avg_words_per_sentence,\n",
        "                        \"Complex Words Count\": num_complex_words,\n",
        "                         \"Word Count\": num_clean_words,\n",
        "                          \"Personal Pronoun Count\": pronoun_count,\n",
        "                          \"Syllable count\":total_syllables})\n",
        "scores_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "PIhZtcaxYieC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort the DataFrame by the \"Filename\" column:\n",
        "scores_df.sort_values(by=\"Filename\", inplace=True)\n",
        "\n",
        "#Reset the index:\n",
        "scores_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Display the sorted DataFrame:\n",
        "scores_df.head()"
      ],
      "metadata": {
        "id": "reecxFMPjaEL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Zq-NKIQysTvn",
        "55-ZJVz3sZyO"
      ],
      "authorship_tag": "ABX9TyMhoYWLaTWPluYaf/1rceTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}